{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    " \n",
    "import math\n",
    "\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History \n",
    "\n",
    "from skimage.io import imread\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a dataset from the Excel sheet\n",
    "def get_dataset(base_dataset, column_name):\n",
    "    return base_dataset[[\"strain_id\",\"strain_image\", column_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the data 70%-train, 30%-test\n",
    "def train_test_split(dataset):\n",
    "    msk = np.random.rand(len(dataset)) < 0.7\n",
    "    train_dataset = dataset[msk].sample(frac=1)\n",
    "    test_dataset = dataset[~msk].sample(frac=1)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize / One Hot Encoding for categorical data\n",
    "def encode_category(dataset, data_column):\n",
    "    one_hot = pd.get_dummies(dataset[data_column])\n",
    "    # Join the encoded df\n",
    "    return dataset.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for loading batches of images\n",
    "def generate_images(dataframe, y_column, batch_size=128):\n",
    "    if y_column == 'env':\n",
    "        classes = dataframe[y_column].unique()\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        batch_input = []\n",
    "        batch_output = []\n",
    "                \n",
    "        sub_dataframe = dataframe.sample(n=batch_size, replace=True) # Get a subset of the dataframe\n",
    "        \n",
    "        for index, row in sub_dataframe.iterrows(): # iterate through each row\n",
    "            \n",
    "            x = row[\"strain_image\"] \n",
    "            img = imread(x)\n",
    "            img = np.expand_dims(img, axis=3)\n",
    "            batch_input.append(img)\n",
    "            \n",
    "            if y_column != 'env':\n",
    "                y = row[y_column]\n",
    "            else:\n",
    "                y = row[classes]\n",
    "                \n",
    "            batch_output.append(y)\n",
    "        \n",
    "        batch_input = np.array(batch_input)\n",
    "        batch_output = np.array(batch_output)\n",
    "        \n",
    "        yield (batch_input, batch_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the model\n",
    "def train_model(dataset, validate_dataset, y_column):\n",
    "    \n",
    "    # ----------------------------------- Metadata -------------------------------------------------\n",
    "    img_x, img_y = 28, 28\n",
    "    input_shape = (img_x, img_y, 1)\n",
    "\n",
    "    batch_size = 128\n",
    "    steps_per_epoch = int(math.ceil(dataset.shape[0] / batch_size))\n",
    "    \n",
    "    num_epoch = 100\n",
    "    val_steps=64\n",
    "    \n",
    "    # ----------------------------------- CNN Architecture -----------------------------------------\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    # Convolution Layer w/ ReLU Activation Function\n",
    "    model.add(Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
    "                     activation='relu',\n",
    "                     input_shape=input_shape))\n",
    "    \n",
    "    # Max Pooling Layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "    \n",
    "    # Convolution Layer w/ ReLU Activation Function\n",
    "    model.add(Conv2D(64, (5, 5), activation='relu'))\n",
    "    \n",
    "    # Max Pooling Layer\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    # Input Layer: connects the convolution and dense layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Hidden Layers\n",
    "    model.add(Dense(1000, kernel_initializer='normal', activation='sigmoid'))\n",
    "    \n",
    "    # Changes the final layer to accommodate for a regression column or categorical\n",
    "    if y_column != 'env':\n",
    "        # Single node output layer with linear activation function\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        \n",
    "        #Uses Mean Squared Error Loss Function and Optimizer\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse'])\n",
    "        \n",
    "    else:\n",
    "        # Gets the amount of classes for the column\n",
    "        class_amount = dataset[y_column].unique().size\n",
    "        \n",
    "        # Adds #[class_amount]-nodes to final layer\n",
    "        model.add(Dense(class_amount, activation='softmax'))\n",
    "        \n",
    "        # Uses cross entropy for categories and NADAM optimizer\n",
    "        model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n",
    "        \n",
    "    model.summary()\n",
    "    \n",
    "    train_gen = generate_images(dataset, y_column, batch_size)\n",
    "    validate_gen = generate_images(validate_dataset, y_column, batch_size)\n",
    "\n",
    "    history = model.fit_generator(train_gen, validation_data=validate_gen,\n",
    "            steps_per_epoch=steps_per_epoch, nb_epoch=num_epoch, verbose=1, validation_steps=val_steps)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # summarize history for loss\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the model and the weights\n",
    "def save_model(model, history, data_column, models_dir, history_dir):\n",
    "    # Save Model in H5 file\n",
    "    model_weights = data_column + \"_model\" + \".h5\"\n",
    "    model_file = join(models_dir, model_weights)\n",
    "\n",
    "    model.save(model_file)\n",
    "    print(\"Saved {} model to disk\".format(data_column))\n",
    "\n",
    "    # Save Model's History to JSON file\n",
    "    history_file = join(history_dir, \"{}_history.json\".format(data_column))\n",
    "    history_dict = history.history\n",
    "    json.dump(history_dict, open(history_file, 'w'))\n",
    "    print(\"Saved {} model's history to disk\".format(data_column))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for creating directory if doesn't exist\n",
    "def get_directory(destination, dir_name):\n",
    "    path = join(save_dir, dir_name)\n",
    "    if not os.path.exists(path):\n",
    "        print(\"Woops, directory doesn't exist. But we gotcha. It exists now!\")\n",
    "        os.makedirs(path)\n",
    "\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_directory = \"Enter path location for directory where the images are stored\"\n",
    "bacteria_workbook = \"Enter path location for excel file\"\n",
    "\n",
    "save_dir = \"Enter a directory to retrieve you models and history\" # Directory for saving things\n",
    "models_dir = get_directory(save_dir, \"models\") # must have a models directory\n",
    "history_dir = get_directory(save_dir, \"history\") # must have a models directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_data = pd.read_excel(bacteria_workbook, dtype=object, sheet_name=\"CNN Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use if want to train model for one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_column = \"Choose a column to analyze\" # e.g. \"toby.max.od\" or \"carb.lag\" or \"env\"\n",
    "dataset = get_dataset(cnn_data, data_column)\n",
    "\n",
    "if data_column == 'env':\n",
    "    # Encodes categories to binary\n",
    "    dataset = encode_category(dataset, data_column)\n",
    "\n",
    "# Gets training and testing set\n",
    "train_dataset, test_dataset = train_test_split(dataset)      \n",
    "\n",
    "# Trains model and returns history object\n",
    "model, history = train_model(dataset=train_dataset, validate_dataset=test_dataset, y_column=data_column)\n",
    "\n",
    "# Save model\n",
    "save_model(model, history, data_column, models_dir, history_dir)\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use if want to train a model for each column in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = cnn_data.columns.values\n",
    "\n",
    "# for column in columns:\n",
    "#     data_column = column # e.g. \"toby.max.od\" or \"carb.lag\" or \"env\"\n",
    "#     dataset = get_dataset(cnn_data, data_column)\n",
    "    \n",
    "#     # Make a CNN model for every column\n",
    "#     if column == 'env':\n",
    "#         # Encodes categories to binary\n",
    "#         dataset = encode_category(dataset, data_column)\n",
    "    \n",
    "#     # Gets training and testing set\n",
    "#     train_dataset, test_dataset = train_test_split(dataset)\n",
    "    \n",
    "#     # Trains model\n",
    "#     model, history = train_model(dataset=train_dataset, validate_dataset=test_dataset, y_column=data_column)\n",
    "    \n",
    "#     # Save model\n",
    "#     save_model(model, history, data_column, models_dir, history_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
